{"title": "Google\u2019s AI thinks this turtle looks like a gun, which is a problem", "content": " \nFrom self-driving cars to smart surveillance cams, society is slowly learning to trust AI over human eyes. But although our new machine vision systems are tireless and ever-vigilant, they\u2019re far from infallible. Just look at the toy turtle above. It looks like a turtle, right? Well, not to a neural network trained by Google to identify everyday objects. To Google\u2019s AI it looks exactly like a rifle. \nThis 3D-printed turtle is an example of what\u2019s known as an \u201cadversarial image.\u201d In the AI world, these are pictures engineered to trick machine vision software, incorporating special patterns that make AI systems flip out. Think of them as optical illusions for computers. You can make adversarial glasses that trick facial recognition systems into thinking you\u2019re someone else, or can apply an adversarial pattern to a picture as a layer of near-invisible static. Humans won\u2019t spot the difference, but to an AI it means that panda has suddenly turned into a pickup truck. \nImagine tricking a self-driving car into seeing stop signs everywhere\nResearching ways of generating and guarding against these sorts of adversarial attacks is an active field of research. And although the attacks are usually strikingly effective, they\u2019re often not too robust. This means that if you rotate an adversarial image or zoom in on it a a little, the computer will see past the pattern and identify it correctly. Why this 3D-printed turtle is significant, though, is because it shows how these adversarial attacks work in the 3D world, fooling a computer when viewed from multiple angles. \n\u201cIn concrete terms, this means it's likely possible that one could construct a yard sale sign which to human drivers appears entirely ordinary, but might appear to a self-driving car as a pedestrian which suddenly appears next to the street,\u201d write labsix, the team of students from MIT who published the research. \u201cAdversarial examples are a practical concern that people must consider as neural networks become increasingly prevalent (and dangerous).\u201d\nLabsix calls their new method \u201cExpectation Over Transformation\u201d and you can read their full paper on it here. As well as creating a turtle that looks like a rifle, they also made a baseball that gets confused for an espresso and numerous non-3D-printed tests. The classes they chose were at random. \nThe group tested their attack against an image classifier developed by Google called Inception-v3. The company makes this freely available for researchers to tinker with, and although it\u2019s not a commercial system, it\u2019s not far from one. Although this attack was not tested against other machine vision software, to date there\u2019s no single fix for adversarial images. When contacted by The Verge, Google did not offer a comment on the paper, but a spokesperson directed us to a number of recent papers outlining ways to foil adversarial attacks that have been published by the company\u2019s researchers.  \n\n\n\n\n\n\n\n\nAn example from labsix of how fragile adversarial attacks often are. The image on the left has been altered so that it\u2019s identified as guacamole. Tilting it slightly means it\u2019s identified once more as a cat.\n\n\nThe research comes with some caveats too. Firstly, the team\u2019s claim that their attack works from \u201cevery angle\u201d isn\u2019t quite right. Their own video demos show that it works from most, but not all angles. Secondly, labsix needed access to Google\u2019s vision algorithm in order to identify its weaknesses and fool it. This is a significant barrier for anyone who would try and use these methods against commercial vision systems deployed by, say, self-driving car companies. However, other adversarial attacks have been shown to work against AI sight-unseen, and, according to Quartz, the labsix team is working on this problem next. \nAdversarial attacks like these aren\u2019t, at present, a big danger to the public. They\u2019re effective, yes, but in limited circumstances. And although machine vision is being deployed more commonly in the real world, we\u2019re not yet so dependent on it that a bad actor with a 3D-printer could cause havoc. The problem is that issues like this exemplify how fragile some AI systems can be. And if we don\u2019t fix these problems now, they could lead to much bigger issues in the future. \nRead more: MAGIC AI: THESE ARE THE OPTICAL ILLUSIONS THAT TRICK, FOOL, AND FLUMMOX COMPUTERS\n\nUpdate November 2nd, 1:20PM ET: The story has been updated with Google\u2019s response. \n"}
