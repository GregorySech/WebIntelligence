{"title": "The machine is learning", "content": " \nEverybody just wants the Star Trek computer.\nWe all know how it works: an ambient AI that's available everywhere, knows nearly everything, and is responsive to our personal needs and desires. It\u2019s always listening, yet it respects our privacy. It's a computer you don't need to think about because no matter how you interact with it \u2014 voice, typing, or touch \u2014 it just does what you want. \nThat's exactly what Google is trying to build. \nIt's just that, like Star Trek, the future is barely hinted at by our current reality.\nAll of the updates to the Google Assistant announced today at Google I/O are just hints. Taken individually, they look a little like the Assistant is playing catch-up: a feature here, support for a new device there, a new developer API over on the side. Assistant is becoming available on the iPhone. Google Home can make outgoing calls. There's new camera and keyboard features. You can buy stuff directly. And a long list of other minor additions.\nEach individual thing is a small convenience today, but none of them \u2014 or even all of them taken together \u2014 come anywhere close to the Starship Enterprise. But step back and you can see the foundation coming together. Instead of trying to rush out big promises about the future, Google's relentless machine is taking a surprisingly measured approach to building the Assistant, betting that AI will slowly overtake everything over time instead of falling into place all at once.\nIt's an approach shaped by fierce competition and early missteps. Google wants to be everywhere, including the iPhone, and it's willing to fight a long fight to get there. \nThe machine is learning.\n \n\n\n\n\n\n\n\nPhoto by Vjeran Pavic / The Verge\n\n\n\nThe iPhone revolution taught us one good lesson and one potentially bad one about how technological change happens.\nThe good lesson: it solidified the cliche that revolutions in computers are driven by a change in the user interface. The command line meant users could see the results of their code immediately. The mouse made those computers widely accessible to non-experts. Multitouch created a world of powerful, always-connected devices that fit in our pockets.\nIn each of these cases, the revolution happened because the gap between input and output got smaller. Interface changes didn't just make computing faster, they made it more immediate, literally removing layers of mediation between you and the computer. The command line meant you didn't have to wait for your punch card to get processed. The mouse meant you could just point to what you wanted. And multitouch on the smartphone meant that the very thing you\u2019re tapping \u2014 the screen \u2014 is the thing that presents information to you. You're touching the information itself.\nTwo of those three input revolutions were popularized by Apple. With Assistant, Google is trying to lay claim to the next one: creating an interface out of AI. If you just look at the new features of Assistant individually, you might miss that they're just emergent features of a larger vision \u2014 reducing the gap between input and output to a mere sliver.\nCan Google make AI the next revolution in I/O?\nThe Google Assistant is meant to someday supplant the icons and text boxes and swipes you currently use, but the AI and machine learning behind it are going to do more than just give you answers: they're going to participate in the interface itself. \nIn a sense, every input revolution was a reduction in the abstraction between you and what the computer is \"thinking.\" With an intelligent assistant, instead of thinking how the interface works or what app you might need, the AI just does it for you. The metaphorical distance between what the computer is doing (AI) and what you're doing (interacting with the computer) will approach zero. \nWhich brings us to the possibly bad lesson the iPhone taught us: that technological revolutions happen quickly. Smartphones took over so fast and so pervasively it made our collective heads spin. But that pace might have been a one-time fluke. It has made us expect intelligent assistants like the Google Assistant, Siri, Alexa, and Cortana to develop and take over at the same rate.\nGiven how complex intelligent assistants are, that's probably not going to happen. So it isn't surprising to see Google slowing things down and tempering expectations. Scott Huffman, VP of Google Assistant, admits that his ambition is still to build that Star Trek computer. \"We're aiming that high again.\" But don't expect it to happen tomorrow, or even this year. As Huffman puts it, \"It will take a while to realize the full fruition.\"\n \n\n\n\n\n\n\n\nPhoto by Vjeran Pavic / The Verge\n\n\n\nAn intelligent assistant that you can use everywhere needs to be available (or at least within shouting distance) of everything. And so Google clearly needed to put it on the most popular single smartphone: the iPhone.\nUsing the Google Assistant on the iPhone is simultaneously an exciting and an underwhelming experience. If you've been using Siri (or, just as likely, have basically been ignoring it), you will be pleasantly surprised at how much better Google is at grabbing information. It knows all your Google stuff, it can control your smart home devices, and it can even send iMessages. \nBut it's not all that different from Siri, and you have to open an app to use it, rather than just holding down your home button. You'll also hit weird limits \u2014 Apple doesn't make its clock app available to third-party apps, so you can't set alarms. \"Siri is pretty good and it is right there,\" says Huffman. \"Things like the alarm clock and things close to the phone, people are going to use Siri.\"\nSo why would you use the Google Assistant instead of Siri? Mostly because you're already a heavy Google user. If you already use Gmail and Google Calendar and even Google Home, the Assistant on the iPhone is automatically tied into that whole ecosystem. When you ask for directions, it takes you to Google Maps and when you set a reminder, it appears in your Google Calendar.\nThe Google Assistant is a different thing from Google Search \u2014 it\u2019s more experimental, so it\u2019s a separate app\nGoogle also made an interesting \u2014 and somewhat surprising \u2014 choice in bringing the Assistant to the iPhone: it's a separate app from Google search. That\u2019s partly about changing expectations, and partly about Google's new strategy of building a foundation for the Assistant that eventually leads to that futuristic computer. \n\"Search is mostly about public information,\" says Huffman. Google has made some half-hearted attempts to get people to use the search box for their personal information, but it never really took; it's just too weird for people. But the Assistant is designed from the jump for personal information, so it was important to separate it from the main Google app. It also works differently. \"The interaction model being a conversational UI from the base is very different from the well-honed, well-oiled UI of search,\" says Huffman.\nIt also probably doesn't hurt that if the Assistant becomes popular, it's another app that Google can get on the prized real estate of the iPhone home screen, alongside YouTube, Facebook, Instagram, and other heavy hitters. Google can\u2019t do anything about the iPhone home button being tied to Siri, but it\u2019s shipping the Assistant app with an associated home screen widget, so you can quickly jump directly to it with a swipe from your home or lock screen.\n \n\n\n\n\n\n\n\nPhoto by Dieter Bohn / The Verge\n\n\n\n\n\nThere are a ton of new updates coming to Google Assistant, the Google Home intelligent speaker, and the Chromecast TV stick later this year:\n\nThe Assistant will accept keyboard input on phones now.\nThe Assistant will have a new camera input that can identify objects and let you ask questions about them.\nGoogle Actions \u2014 the thing that lets the Assistant talk to third-party services like Alexa skills \u2014 are going to be available on phones. \nYou can call any phone number for free in the US and Canada with Google Home. \n70 different smart home manufacturers will work with Google Home now, with an open system to add more themselves whenever.\nGoogle Home can add calendar events and reminders, and will pulse a light when a reminder is waiting for you.\nSpotify\u2019s free tier it coming to Google Home, as are Deezer and SoundCloud.\n\"A bunch\" of third-party hardware makers will announce Google Home speakers.\nThe Chromecast is going to have a bare-bones television UI you can use to pick what show you want to watch.\nYou can stream your Nest security cam video to Chromecast.\nYou can also tell Google Home to watch HBO or Hulu on Chromecast. \nGoogle Home will be able to send visual search results to the Chromecast when appropriate \u2014 so you can ask to see your calendar for the day, and it will just show you.\nYou can finally set reminders and calendar events on Home, too. \nGoogle Home is coming to more countries: Australia, Canada, France, Germany, and Japan. \nGoogle Assistant will work in more languages: Brazilian, Portuguese, French, German, and Japanese. Later this year Google will add Italian, Korean, and Spanish.\n\n\n\nFor the time being, Google isn't going to get in the game of trying to take on Alexa in a heads-up battle for the biggest number of apps (or, in home speaker parlance: skills or actions). \nInstead, it has a radically different idea for the foundation of how an assistant should work: the web. \n\"Rather than managing apps and installing things,\" Huffman argues. \"I think of it a little bit like the web. You never say, \u2018Do you have website x?\u2019 \u2026 I have a browser so I have them all. That's the way it should work.\" Rishi Chandra, VP of Google Home, agrees. \"Our context is always the scale of what the web can do ... Otherwise everyone's thinking way too small.\"\n\"What do people care about?\" asks Valerie Nygaard, product manager for Google Assistant. \"They care about what [assistants] can help them do.\" So instead of a big list of skills you look for and enable like you do on Alexa, the Assistant just tries to be ready to give you anything you ask for, sort of like a web browser. \n\"We want to look at the whole spectrum of what we can do. We look at what our competitors can do, and what we can do, and we take that very seriously,\" says Nygaard.\nSo Google's argument is really that while Amazon has a commanding early lead with specific skills, what it's really trying to do is build the beginnings of an entirely new kind of AI-assisted internet that's more open than Alexa or Siri \u2014 though not nearly as open as the web itself. Rather than try to craft partnerships with every single app maker like Apple does for Siri, Google is just opening up a place where any smart home device maker can register their product to work. Rather than ask everybody to make a custom \"skill\" like Amazon does with Alexa, Google is hoping its Assistant can figure it out using Google's own ranking algorithms and what it knows about you. \nNygaard showed me a demo of how that system will work by ordering delivery from Panera. She asked the Assistant to do it, and Panera's chatbot \u2014 not dissimilar from what we've seen on Facebook Messenger \u2014 spun up. She could speak to it normally to order, but that conversation was augmented by the Assistant. Google knew her address, so she could just tap it to send to Panera. And rather than setting up some kind of weirdo Panera account, she could just pay with the credit card information Google had on file. \"I'm in control of that, and I'm absolutely in control of what I share with Panera,\" Nygaard said.\nAll of this means that Google has a wholly different problem than Amazon or Apple: most users might not even realize what they can do with it. \"The biggest thing I'm worried about [is] how do we educate people about what you can do?\" Chandra asks. \nThe answer, honestly, is still really unclear.\n \n\n\n\n\n\n\n\nPhoto by Dieter Bohn / The Verge\n\n\n\nThis is not the first time we've seen grand visions of the future of intelligent computing from Google. First there was Google Now, which attempted to provide proactive information before you even asked for it. Then there was Now on Tap, which tried to put Google's intelligence to work understanding apps so you wouldn't have to manage the information inside them.\nBoth failed to live up to their promise. Both have been quietly and somewhat unceremoniously folded into Google's latest effort, the Google Assistant. \"The assistant umbrella gives us a place to put all those things that makes sense,\" says Huffman. The problem with Now and Now on Tap, Chandra says, is that on their own they didn't comprise a complete vision for what an AI computer should do. Instead, they were just \"features of that broader vision.\" Huffman calls them \"different aspects of what that AI-based UX needs to do.\"\nHopes are higher that this time around, the Assistant is the right foundation. For one thing, it's more flexible. You can interact with it on multiple devices: Android phones, iPhones, watches, televisions, and cars. It will show you your history of conversational interactions so you don't lose what you were looking for. You can speak to it or (finally) type to it. \n\u201cWe don't think there's a typing assistant and a voice assistant. We think there's one assistant.\u201d \nLater this year, you'll be able to hit a button and use the camera to identify things and ask questions about it. In the demo Google is showing, you can point it at a theater marquee, it will identify the band scheduled to play, and you can choose to do whatever you want with that information, like buy tickets, save it to your calendar, or watch music videos on YouTube.\nAnother reason that the Assistant has a better shot is that Google is being more careful about building it out. It recently added multiuser functionality to Google Home, so it could identify who was speaking to it and take action based on that person's Google account. The company waited for that before it launched reminders and calendar events, rather that put those features out earlier. Reminders won't make noise at first; they'll gently pulse the light on the Home. \nAnd for those of us who use Google Apps for work, those will eventually come to Home, too \u2014 once Google can work through some of the thornier legal issues of making your work information available on a personal home device.\nPeople tend to be more skeptical of Google when it comes to privacy. And for good reason: this is a company that fundamentally tries to learn everything it can about you so it can serve ads. But the company consistently insists that neither ads nor other revenue plans are on the radar just yet. \"We want to build the great consumer ecosystem and developer ecosystem before we're worried about our cut,\" says Nygaard.\nPrivacy concerns also apply to the calling feature \u2014 you can dial out, but the Home itself won't let people call it. When you do call, it defaults to a private number unless you specifically tell Google to use your mobile phone number. Incoming calls may happen later, but \"it's one of the things we only want to do once we can get it right,\" according to Chandra.\n\"When you have one shot at giving an answer\u2026 the bar gets higher.\u201d\nAll of that caution doesn't mean that Google deserves a free pass, though. Even when it's not about privacy and security, Google has already shown it can make mistakes with Home. The Assistant's tendency to just provide the first answer it can find from the web has led it to spout off some horrible misinformation and inadvertently become the vehicle for burger ads. In a recent blog post, Google has announced changes to mitigate these problems \u2014 but they're not going to entirely go away. \"When you have one shot at giving an answer\u2026 the bar gets higher,\" says Chandra. \"This is a Google-wide challenge. We are 100 percent treating it seriously, one of [CEO] Sundar [Pichai]\u2019s top things is making sure we're as accurate as possible.\u201d \nBut \u201cit's going to be algorithmic, so we can't be perfect,\" he says. Huffman notes that Google is working to make \"attribution really clear\" when it provides that single answer. The Google Home may also tell users that more information about a given topic is available in the Google Home app on the phone. It's not a complete fix, but it's at least a start. \n \n\n\n\n\n\n\n\nPhoto by Vjeran Pavic / The Verge\n\n\n\nHere's one more example of caution: nearly nothing described here is going to be available until later this year. Even though it holds the Google I/O developer event every May, the company doesn't really hold much to announce there, nor does it rush to release products on that deadline. Instead, that's just how Google rolls, iterating its products so often throughout the year that you might not even notice they have new features.\nHuffman says that Google learned an important lesson from search: people just keep asking questions Google can't answer yet, so Google goes out and tries to answer those questions. \"What naturally happens is they just think it should do everything and they keep asking. And we learn over time what to do next,\" he says. If Google releases enough Google Assistant features it might make it good enough for you to keep using it, even if it doesn't realize its total potential for a long time to come. \nThat steady iteration is also a strategy for setting expectations. If users know that more updates are coming, they might temper their \"future of computing\" expectations when they use assistants today. Huffman will tell you that \"what we're aiming for is the assistant that you can have any conversation with on any device and it will do anything.\" But in the same breath he'll tell you that we're nowhere near that yet. \"This isn't a six-month problem, it isn't even a one-year problem. It's a multi-year thing,\" he says.\nThat's the thing about the Star Trek computer: it's always in the future. \n\n"}
