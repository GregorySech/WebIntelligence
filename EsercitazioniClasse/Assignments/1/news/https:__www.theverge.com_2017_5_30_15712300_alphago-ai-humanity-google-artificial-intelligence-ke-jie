{"title": "AI, the humanity!", "content": " \nA loss for humanity! Man succumbs to machine! \nIf you heard about AlphaGo\u2019s latest exploits last week \u2014 crushing the world\u2019s best Go player and confirming that artificial intelligence had mastered the ancient Chinese board game \u2014 you may have heard the news delivered in doomsday terms.\nThere was a certain melancholy to Ke Jie\u2019s capitulation, to be sure. The 19-year-old Chinese prodigy declared he would never lose to an AI following AlphaGo\u2019s earthshaking victory over Lee Se-dol last year. To see him onstage last week, nearly bent double over the Go board and fidgeting with his hair, was to see a man comprehensively put in his place.\nBut focusing on that would miss the point. DeepMind, the Google-owned company that developed AlphaGo, isn\u2019t attempting to crush humanity \u2014 after all, the company is made up of humans itself. AlphaGo represents a major human achievement and the takeaway shouldn\u2019t be that AI is surpassing our abilities, but instead that AI will enhance our abilities.\n\n\n\n\n\n\n\n\nDemis Hassabis, DeepMind co-founder and CEO.\n\n\nWhen speaking to DeepMind and Google developers at the Future of Go Summit in Wuzhen, China last week, I didn\u2019t hear much about the four games AlphaGo won over Lee Se-dol last year. Instead, I heard a lot about the one that it lost.\n\u201cWe were interested to see if we could fix the problems, the knowledge gaps as we call them, that Lee Se-dol brilliantly exposed in game four with his incredible win, showing that there was a weakness in AlphaGo\u2019s knowledge,\u201d DeepMind co-founder and CEO Demis Hassabis said on the first day of the event. \u201cWe worked hard to see if we could fix that knowledge gap and actually teach, or have AlphaGo learn itself, how to deal with those kinds of positions. We\u2019re confident now that AlphaGo is better in those situations, but again we don\u2019t know for sure until we play against an amazing master like Ke Jie.\u201d\n\u201cAlphaGo Master has become its own teacher.\u201d\nAs it happened, AlphaGo steamrolled Ke into a 3-0 defeat, suggesting that those knowledge gaps have been closed. It\u2019s worth noting, however, that DeepMind had to learn from AlphaGo\u2019s past mistakes to reach this level. If the AI had stood still for the past year, it\u2019s entirely possible that Ke would have won; he\u2019s a far stronger player than Lee. But AlphaGo did not stand still.\nThe version of AlphaGo that played Ke has been completely rearchitected \u2014 DeepMind calls it AlphaGo Master. \u201cThe main innovation in AlphaGo Master is that it\u2019s become its own teacher,\u201d says Dave Silver, DeepMind\u2019s lead researcher on AlphaGo. \u201cSo [now] AlphaGo actually learns from its own searches to improve its neural networks, both the policy network and the value network, and this makes it learn in a much more general way. One of the things we\u2019re most excited about is not just that it can play Go better but we hope that this\u2019ll actually lead to technologies that are more generally applicable to other challenging domains.\u201d\n\n\n\n\nAlphaGo is comprised of two networks: a policy network that selects the next move to play, and a value network that analyzes the probability of winning. The policy network was initially based on millions of historical moves from actual games played by Go professionals. But AlphaGo Master goes much further by searching through the possible moves that could occur if a particular move is played, increasing its understanding of the potential fallout.\n\u201cThe original system played against itself millions of times, but it didn\u2019t have this component of using the search,\u201d Hassabis tells The Verge. \u201c[AlphaGo Master is] using its own strength to improve its own predictions. So whereas in the previous version it was mostly about generating data, in this version it\u2019s actually using the power of its own search function and its own abilities to improve one part of itself, the policy net.\u201d Essentially, AlphaGo is now better at assessing why a particular move would be the strongest possible option.\n\u201cThe whole idea is to reduce your reliance on that human bootstrapping step.\u201d\nI asked Hassabis whether he thought this system could work without the initial dataset taken from historical games of Go. \u201cWe\u2019re running those tests at the moment and we\u2019re pretty confident, actually,\u201d he said. \u201cThe initial results have been that it\u2019s looking pretty good. That\u2019ll be part of this future paper that we\u2019re going to publish, so we\u2019re not talking about that at the moment, but it\u2019s looking promising. The whole idea is to reduce your reliance on that human bootstrapping step.\u201d\nBut in order to defeat Ke, DeepMind needed to fix the weaknesses in the original AlphaGo that Lee exposed. Although the AI gets ever stronger by playing against itself, DeepMind couldn\u2019t rely on that baseline training to cover the knowledge gaps \u2014 nor could it hand-code a solution. \u201cIt\u2019s not like a traditional program where you just fix a bug,\u201d says Hassabis, who believes that similar knowledge gaps are likely to be a problem faced by all kinds of learning systems in the future. \u201cYou have to kind of coax it to learn new knowledge or explore that new area of the domain, and there are various strategies to do that. You can use adversarial opponents that push you into exploring those spaces, and you can keep different varieties of the AlphaGo versions to play each other so there\u2019s more variety in the player pool.\u201d \n\u201cAnother thing we did is when we assessed what kinds of positions we thought AlphaGo had a problem with, we looked at the self-play games and we identified games algorithmically \u2014 we wrote another algorithm to look at all those games and identify places where AlphaGo seemed to have this kind of problem. So we have a library of those sorts of positions, and we can test our new systems not only against each other in the self-play but against this database of known problematic positions, so then we could quantify the improvement against that.\u201d\n\n\n\n\n\n\n\n\nDave Silver, DeepMind\u2019s lead researcher on AlphaGo.\n\n\nNone of this increase in performance has required an increase in power. In fact, AlphaGo Master uses much less power than the version of AlphaGo that beat Lee Se-dol; it runs on a single second-gen Tensor Processing Unit machine in the Google Cloud, whereas the previous version used 50 TPUs at once. \u201cYou shouldn\u2019t think of this as running on compute power that\u2019s beyond the access of normal people,\u201d says Silver. \u201cThe special thing about it is the algorithm that\u2019s being used as opposed to the amount of compute.\u201d\nAlphaGo learned from humans, and humans are learning from AlphaGo\nAlphaGo is learning from humans, then, even if it may not need to in the future. And in turn, humans have learned from AlphaGo. The simplest demonstration of this came in Ke Jie\u2019s first match against the AI, where he used a 3-3 point as part of his opening strategy. That\u2019s a move that fell out of favor over the past several decades, but it\u2019s seen a resurgence in popularity after AlphaGo employed it to some success. And Ke pushed AlphaGo to its limits in the second game; the AI determined that his first 50 moves were \u201cperfect,\u201d and his first 100 were better than anyone had ever played against the Master version.\nAlthough the Go community might not necessarily understand why a given AlphaGo move works in the moment, the AI provides a whole new way to approach the game. Go has been around for thousands of years, and AlphaGo has sparked one of the most profound shifts yet in how the game is played and studied. \nBut if you\u2019re reading this in the West, you probably don\u2019t play Go. What can AlphaGo do for you?\n\n\n\n\n\n\n\n\nAndrew Jackson and Lee Ha-jin discuss AlphaGo's first game against Ke Jie.\n\n\nSay you\u2019re a data center architect working at Google. It\u2019s your job to make sure everything runs efficiently and coolly. To date, you\u2019ve achieved that by designing the system so that you\u2019re running as few pieces of cooling equipment at once as possible \u2014 you turn on the second piece only after the first is maxed out, and so on. This makes sense, right? Well, a variant of AlphaGo named Dr. Data disagreed. \n\u201cWhat Dr. Data decided to do was actually turn on as many units as possible and run them at a very low level,\u201d Hassabis says. \u201cBecause of the switching and the pumps and the other things, that turned out to be better \u2014 and I think they\u2019re now taking that into new data center designs, potentially. They\u2019re taking some of those ideas and reincorporating them into the new designs, which obviously the AI system can\u2019t do. So the human designers are looking at what the AlphaGo variant was doing, and then that\u2019s informing their next decisions.\u201d Dr. Data is at work right now in Google\u2019s data centers, saving the company 40 percent in electricity required for cooling and resulting in 15 percent overall less energy usage. \nDeepMind believes that the same principle will apply to science and health care, with deep-learning techniques helping to improve the accuracy and efficiency of everything from protein-folding to radiography. Perhaps less ambitiously but no less importantly, it may also lead to more sensible workflows. \u201cYou can imagine across a hospital or many hospitals you might be able to figure out that there\u2019s this process one hospital\u2019s using, or one nurse is using, that\u2019s super effective over time,\u201d says Hassabis. \u201cMaybe they\u2019re doing something slightly different to this other hospital, and perhaps the other hospital can learn from that. I think at the moment you\u2019d never know that was happening, but you can imagine that an AI system might be able to pick up on that and share that knowledge effectively between different doctors and hospitals so they all end up with the best practice.\u201d\n\n\n\n\n\n\n\n\nThese are areas particularly fraught with roadblocks and worries for many, of course. And it\u2019s natural for people to be suspicious of AI \u2014 I experienced it myself somewhat last week. My hotel was part of the same compound as the Future of Go Summit, and access to certain areas was gated by Baidu\u2019s machine learning-powered facial recognition tech. It worked instantly, every time, often without me even knowing where the camera was; I\u2019d just go through the gate and see my Verge profile photo flash up on a screen. I never saw it fail for the thousands of other people at the event, either. And all of this worked based on nothing more than a picture of me taken on an iPad at check-in. \nI know that Facebook and Google and probably tons of other companies also know what I look like. But the weird feeling I got from seeing my face flawlessly recognized multiple times a day for a week shows that companies ought to be sensitive about the way they roll out AI technologies. It also, to some extent, probably explains why so many people seem unsettled by AlphaGo\u2019s success.\nBut again, that success is a success built by humans. AlphaGo is already demonstrating the power of what can happen not only when AI learns from us, but when we learn from AI. At this stage, it\u2019s technology worth being optimistic about.\nPhotography by Sam Byford / The Verge\n"}
