{"title": "This backflipping noodle has a lot to teach us about AI safety", "content": " \nAI isn\u2019t going to be a threat to humanity because it\u2019s evil or cruel, AI will be a threat to humanity because we haven\u2019t properly explained what it is we want it to do. Consider the classic \u201cpaperclip maximizer\u201d thought experiment, in which an all-powerful AI is told, simply, \u201cmake paperclips.\u201d The AI, not constrained by any human morality or reason, does so, eventually transforming all resources on Earth into paperclips, and wiping out our species in the process. As with any relationship, when talking to our computers, communication is key.\nThat\u2019s why a new piece of research published yesterday by Google\u2019s DeepMind and the Elon Musk-funded OpenAI institute is so interesting. It offers a simple way for humans to give feedback to AI systems \u2014 crucially, without the instructor needing to know anything about programming or artificial intelligence.\nThe method is a variation of what\u2019s known as \u201creinforcement learning\u201d or RL. With RL systems, a computer learns by trial-and-error, repeating the same task over and over, while programmers direct its actions by setting certain reward criteria. For example, if you want a computer to learn how to play Atari games (something DeepMind has done in the past) you might make the game\u2019s point system the reward criteria. Over time, the algorithm will learn to play in a way that best accrues points, often leading to super-human performance. \nWhat DeepMind and OpenAI\u2019s researchers have done is replace this predefined reward criteria with a much simpler feedback system. Humans are shown an AI performing two versions of the same task and simply tell it which is better. This happens again and again, and eventually the systems learns what is expected of it. Think of it like getting an eye test, when you\u2019re looking through different lenses, and being asked over and over: better... or worse? Here\u2019s what that looks like when teaching a computer to play the classic Atari game Q*bert:\n\n\n\n\n\n\n\n\n\nThis method of feedback is surprisingly effective, and researchers were able to use it to train an AI to play a number of Atari video games, as well perform simulated robot tasks (like picking telling an arm to pick up a ball). This better / worse reward function could even be used to program trickier behavior, like teaching a very basic virtual robot how to backflip. That\u2019s how we get to the GIF at the top of the page. The behavior you see has been created by watching the \u201cHopper\u201d bot jump up and down, and telling it \u201cwell done\u201d when it gets a bit closer to doing a backflip. Over time, it learns how. \nOf course, no one is suggesting this method is a cure-all for teaching AI. There are a number of big downsides and limitations in using this sort of feedback. The first being that although it doesn\u2019t take much skill on behalf of the human operator, it does take time. For example, in teaching the \u201cHopper\u201d bot to backflip, a human was asked to judge its behavior some 900 times \u2014 a process that took about an hour. The bot itself had to work through 70 hours of simulated training time, which was sped up artificially.\nFor some simple tasks, says Oxford Robotics researcher Markus Wulfmeier (who was not involved in this research), it would be quicker for a programmer to simply define what it is they wanted. But, says Wulfmeier, it\u2019s \u201cincreasingly important to render human supervision more effective\u201d for AI systems, and this paper \u201crepresents a small step in the right direction.\u201d \nDeepMind and OpenAI say pretty much the same \u2014 it\u2019s a small step, but a promising one, and in the future, they\u2019re looking to apply it to more and more complex scenarios. Speaking to The Verge over email, DeepMind researcher Jan Leike said: \u201cThe setup described in [our paper] already scales from robotic simulations to more complex Atari games, which suggests that the system will scale further.\u201d Leike suggests the next step is to test it in more varied 3D environments. You can read the full paper describing the work here. \n"}
