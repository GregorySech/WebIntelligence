{"title": "The invention of AI \u2018gaydar\u2019 could be the start of something much worse", "content": " \nTwo weeks ago, a pair of researchers from Stanford University made a startling claim. Using hundreds of thousands of images taken from a dating website, they said they had trained a facial recognition system that could identify whether someone was straight or gay just by looking at them. The work was first covered by The Economist, and other publications soon followed suit, with headlines like \u201cNew AI can guess whether you're gay or straight from a photograph\u201d and \u201cAI Can Tell If You're Gay From a Photo, and It's Terrifying.\u201d\nAs you might have guessed, it\u2019s not as straightforward as that. (And to be clear, based on this work alone, AI can\u2019t tell whether someone is gay or straight from a photo.) But the research captures common fears about artificial intelligence: that it will open up new avenues for surveillance and control, and could be particularly harmful for marginalized people. One of the paper\u2019s authors, Dr Michal Kosinski, says his intent is to sound the alarm about the dangers of AI, and warns that facial recognition will soon be able to identify not only someone\u2019s sexual orientation, but their political views, criminality, and even their IQ. \nsome warn we\u2019re replacing the calipers of physiognomy with neural networks\nWith statements like these, some worry we\u2019re reviving an old belief with a bad history: that you can intuit character from appearance. This pseudoscience, physiognomy, was fuel for the scientific racism of the 19th and 20th centuries, and gave moral cover to some of humanity\u2019s worst impulses: to demonize, condemn, and exterminate fellow humans. Critics of Kosinski\u2019s work accuse him of replacing the calipers of the 19th century with the neural networks of the 21st, while the professor himself says he is horrified by his findings, and happy to be proved wrong. \u201cIt\u2019s a controversial and upsetting subject, and it\u2019s also upsetting to us,\u201d he tells The Verge. \nBut is it possible that pseudoscience is sneaking back into the world, disguised in new garb thanks to AI? Some people say machines are simply able to read more about us than we can ourselves, but what if we\u2019re training them to carry out our prejudices, and, in doing so, giving new life to old ideas we rightly dismissed? How are we going to know the difference? \nCan AI really spot sexual orientation? \nFirst, we need to look at the study at the heart of the recent debate, written by Kosinski and his co-author Yilun Wang. Its results have been poorly reported, with a lot of the hype coming from misrepresentations of the system\u2019s accuracy. The paper states: \u201cGiven a single facial image, [the software] could correctly distinguish between gay and heterosexual men in 81 percent of cases, and in 71 percent of cases for women.\u201d These rates increase when the system is given five pictures of an individual: up to 91 percent for men, and 83 percent for women. \nOn the face of it, this sounds like \u201cAI can tell if a man is gay or straight 81 percent of the time by looking at his photo.\u201d (Thus the headlines.) But that\u2019s not what the figures mean. The AI wasn\u2019t 81 percent correct when being shown random photos: it was tested on a pair of photos, one of a gay person and one of a straight person, and then asked which individual was more likely to be gay. It guessed right 81 percent of the time for men and 71 percent of the time for women, but the structure of the test means it started with a baseline of 50 percent \u2014 that\u2019s what it\u2019d get guessing at random. And although it was significantly better than that, the results aren\u2019t the same as saying it can identify anyone\u2019s sexual orientation 81 percent of the time. \n\u201cPeople are scared of a situation where [you\u2019re in a crowd] and a computer identifies whether you\u2019re gay.\u201d\nAs Philip Cohen, a sociologist at the University of Maryland who wrote a blog post critiquing the paper, told The Verge: \u201cPeople are scared of a situation where you have a private life and your sexual orientation isn\u2019t known, and you go to an airport or a sporting event and a computer scans the crowd and identifies whether you\u2019re gay or straight. But there\u2019s just not much evidence this technology can do that.\u201d \nKosinski and Wang make this clear themselves toward the end of the paper when they test their system against 1,000 photographs instead of two. They ask the AI to pick out who is most likely to be gay in a dataset in which 7 percent of the photo subjects are gay, roughly reflecting the proportion of straight and gay men in the US population. When asked to select the 100 individuals most likely to be gay, the system gets only 47 out of 70 possible hits. The remaining 53 have been incorrectly identified. And when asked to identify a top 10, nine are right. \nIf you were a bad actor trying to use this system to identify gay people, you couldn\u2019t know for sure you were getting correct answers. Although, if you used it against a large enough dataset, you might get mostly correct guesses. Is this dangerous? If the system is being used to target gay people, then yes, of course. But the rest of the study suggests the program has even further limitations.  \nWhat can computers really see that humans can\u2019t?\nIt\u2019s also not clear what factors the facial recognition system is using to make its judgements. Kosinski and Wang\u2019s hypothesis is that it\u2019s primarily identifying structural differences: feminine features in the faces of gay men and masculine features in the faces of gay women. But it\u2019s possible that the AI is being confused by other stimuli \u2014 like facial expressions in the photos. \nThe AI might be identifying stereotypes, not biological differences\nThis is particularly relevant because the images used in the study were taken from a dating website. As Greggor Mattson, a professor of sociology at Oberlin College, pointed out in a blog post, this means that the images themselves are biased, as they were selected specifically to attract someone of a certain sexual orientation. They almost certainly play up to our cultural expectations of how gay and straight people should look, and, to further narrow their applicability, all the subjects were white, with no inclusion of bisexual or self-identified trans individuals. If a straight male chooses the most stereotypically \u201cmanly\u201d picture of himself for a dating site, it says more about what he thinks society wants from him than a link between the shape of his jaw and his sexual orientation. \nTo try and ensure their system was looking at facial structure only, Kosinski and Wang used software called VGG-Face, which encodes faces as strings of numbers and has been used for tasks like spotting celebrity lookalikes in paintings. This program, they write, allows them to \u201cminimize the role [of] transient features\u201d like lighting, pose, and facial expression. \nBut researcher Tom White, who works on AI facial system, says VGG-Face is actually very good at picking up on these elements. White pointed this out on Twitter, and explained to The Verge over email how he\u2019d tested the software and used it to successfully distinguish between faces with expressions like \u201cneutral\u201d and \u201chappy,\u201d as well as poses and background color.\n\n\n\n\n\n\n\n\nA figure from the paper showing the average faces of the participants, and the difference in facial structures that they identified between the two sets. \nImage: Kosinski and Wang\n\n\nSpeaking to The Verge, Kosinski says he and Wang have been explicit that things like facial hair and makeup could be a factor in the AI\u2019s decision-making, but he maintains that facial structure is the most important. \u201cIf you look at the overall properties of VGG-Face, it tends to put very little weight on transient facial features,\u201d Kosinski says. \u201cWe also provide evidence that non-transient facial features seem to be predictive of sexual orientation.\u201d\nThe problem is, we can\u2019t know for sure. Kosinski and Wang haven\u2019t released the program they created or the pictures they used to train it. They do test their AI on other picture sources, to see if it\u2019s identifying some factor common to all gay and straight, but these tests were limited and also drew from a biased dataset \u2014 Facebook profile pictures from men who liked pages such as \u201cI love being Gay,\u201d and \u201cGay and Fabulous.\u201d\nDo men in these groups serve as reasonable proxies for all gay men? Probably not, and Kosinski says it\u2019s possible his work is wrong. \u201cMany more studies will need to be conducted to verify [this],\u201d he says. But it\u2019s tricky to say how one could completely eliminate selection bias to perform a conclusive test. Kosinski tells The Verge, \u201cYou don\u2019t need to understand how the model works to test whether it\u2019s correct or not.\u201d However, it\u2019s the acceptance of the opacity of algorithms that makes this sort of research so fraught. \nIf AI can\u2019t show its working, can we trust it?\nAI researchers can\u2019t fully explain why their machines do the things they do. It\u2019s a challenge that runs through the entire field, and is sometimes referred to as the \u201cblack box\u201d problem. Because of the methods used to train AI, these programs can\u2019t show their work in the same way normal software does, although researchers are working to amend this.\nIn the meantime, it leads to all sorts of problems. A common one is that sexist and racist biases are captured from humans in the training data and reproduced by the AI. In the case of Kosinski and Wang\u2019s work, the \u201cblack box\u201d allows them to make a particular scientific leap of faith. Because they\u2019re confident their system is primarily analyzing facial structures, they say their research shows that facial structures predict sexual orientation. (\u201cStudy 1a showed that facial features extracted by a [neural network] can be used to accurately identify the sexual orientation of both men and women.\")\n\u201cBiology\u2019s a little bit more nuanced than we often give it credit for.\u201d\nExperts say this is a misleading claim that isn\u2019t supported by the latest science. There may be a common cause for face shape and sexual orientation \u2014 the most probable cause is the balance of hormones in the womb \u2014 but that doesn\u2019t mean face shape reliably predicts sexual orientation, says Qazi Rahman, an academic at King\u2019s College London who studies the biology of sexual orientation. \u201cBiology\u2019s a little bit more nuanced than we often give it credit for,\u201d he tells The Verge. \u201cThe issue here is the strength of the association.\u201d \nThe idea that sexual orientation comes primarily from biology is itself controversial. Rahman, who believes that sexual orientation is mostly biological, praises Kosinski and Wang\u2019s work. \u201cIt\u2019s not junk science,\u201d he says. \u201cMore like science someone doesn\u2019t like.\u201d But when it comes to predicting sexual orientation, he says there\u2019s a whole package of \u201catypical gender behavior\u201d that needs to be considered. \u201cThe issue for me is more that [the study] misses the point, and that\u2019s behavior.\u201d\n\n\n\n\n\n\n\n\nIs there a gay gene? Or is sexuality equally shaped by society and culture?\n\n\nReducing the question of sexual orientation to a single, measurable factor in the body has a long and often inglorious history. As Matton writes in his blog post, approaches have ranged from \u201c19th century measurements of lesbians\u2019 clitorises and homosexual men\u2019s hips, to late 20th century claims to have discovered \u2018gay genes,\u2019 \u2018gay brains,\u2019 \u2018gay ring fingers,\u2019 \u2018lesbian ears,\u2019 and \u2018gay scalp hair.\u2019\u201d The impact of this work is mixed, but at its worst it\u2019s a tool of oppression: it gives people who want to dehumanize and persecute sexual minorities a \u201cscientific\u201d pretext. \nJenny Davis, a lecturer in sociology at the Australian National University, describes it as a form of biological essentialism. This is the belief that things like sexual orientation are rooted in the body. This approach, she says, is double-edged. On the one hand, it \u201cdoes a useful political thing: detaching blame from same-sex desire. But on the other hand, it reinforces the devalued position of that kind of desire,\u201d setting up hetrosexuality as the norm and framing homosexuality as \u201cless valuable \u2026 a sort of illness.\u201d \nAnd it\u2019s when we consider Kosinski and Wang\u2019s research in this context that AI-powered facial recognition takes on an even darker aspect \u2014 namely, say some critics, as part of a trend to the return of physiognomy, powered by AI. \nYour character, as plain as the nose on your face\nFor centuries, people have believed that the face held the key to the character. The notion has its roots in ancient Greece, but was particularly influential in the 19th century. Proponents of physiognomy suggested that by measuring things like the angle of someone\u2019s forehead or the shape of their nose, they could determine if a person was honest or a criminal. Last year in China, AI researchers claimed they could do the same thing using facial recognition.\nTheir research, published as \u201cAutomated Inference on Criminality Using Face Images,\u201d caused a minor uproar in the AI community. Scientists pointed out flaws in the study, and concluded that that work was replicating human prejudices about what constitutes a \u201cmean\u201d or a \u201cnice\u201d face. In a widely shared rebuttal titled \u201cPhysiognomy\u2019s New Clothes,\u201d Google researcher Blaise Ag\u00fcera y Arcas and two co-authors wrote that we should expect \u201cmore research in the coming years that has similar \u2026 false claims to scientific objectivity in order to \u2018launder\u2019 human prejudice and discrimination.\u201d (Google declined to make Ag\u00fcera y Arcas available to comment on this report.)\n\n\n\n\n\n\n\n\nAn illustration of physiognomy from Giambattista della Porta\u2019s De humana physiognomonia\n\n\n\nKosinski and Wang\u2019s paper clearly acknowledges the dangers of physiognomy, noting that the practice \u201cis now universally, and rightly, rejected as a mix of superstition and racism disguised as science.\u201d But, they continue, just because a subject is \u201ctaboo,\u201d doesn\u2019t mean it has no basis in truth. They say that because humans are able to read characteristics like personality in other people\u2019s faces with \u201clow accuracy,\u201d machines should be able to do the same but more accurately. \nKosinski says his research isn\u2019t physiognomy because it\u2019s using rigorous scientific methods, and his paper cites a number of studies showing that we can deduce (with varying accuracy) traits about people by looking at them. \u201cI was educated and made to believe that it\u2019s absolutely impossible that the face contains any information about your intimate traits, because physiognomy and phrenology were just pseudosciences,\u201d he says. \u201cBut the fact that they were claiming things without any basis in fact, that they were making stuff up, doesn\u2019t mean that this stuff is not real.\u201d He agrees that physiognomy is not science, but says there may be truth in its basic concepts that computers can reveal. \nAI\u2019s intelligence isn\u2019t artificial: it\u2019s human\nFor Davis, this sort of attitude comes from a widespread and mistaken belief in the neutrality and objectivity of AI. \u201cArtificial intelligence is not in fact artificial,\u201d she tells The Verge. \u201cMachines learn like humans learn. We\u2019re taught through culture and absorb the norms of social structure, and so does artificial intelligence. So it will re-create, amplify, and continue on the trajectories we\u2019ve taught it, which are always going to reflect existing cultural norms.\u201d\nWe\u2019ve already created sexist and racist algorithms, and these sorts of cultural biases and physiognomy are really just two sides of the same coin: both rely on bad evidence to judge others. The work by the Chinese researchers is an extreme example, but it\u2019s certainly not the only one. There\u2019s at least one startup already active that claims it can spot terrorists and pedophiles using face recognition, and there are many others offering to analyze \u201cemotional intelligence\u201d and conduct AI-powered surveillance. \nFacing up to what\u2019s coming\nBut to return to the questions implied by those alarming headlines about Kosinski and Wang\u2019s paper: is AI going to be used to persecute sexual minorities? \nThis system? No. A different one? Maybe. \nKosinski and Wang\u2019s work is not invalid, but its results need serious qualifications and further testing. Without that, all we know about their system is that it can spot with some reliability the difference between self-identified gay and straight white people on one particular dating site. We don\u2019t know that it\u2019s spotted a biological difference common to all gay and straight people; we don\u2019t know if it would work with a wider set of photos; and the work doesn\u2019t show that sexual orientation can be deduced with nothing more than, say, a measurement of the jaw. It\u2019s not decoded human sexuality any more than AI chatbots have decoded the art of a good conversation. (Nor do its authors make such a claim.) \n\n\n\n\n\n\n\n\nStartup Faception claims it can identify how likely people are to be terrorists just by looking at their face. \nImage: Faception\n\n\nThe research was published to warn people, say Kosinski, but he admits it\u2019s an \u201cunavoidable paradox\u201d that to do so you have to explain how you did what you did. All the tools used in the paper are available for anyone to find and put together themselves. Writing at the deep learning education site Fast.ai, researcher Jeremy Howard concludes: \u201cIt is probably reasonably [sic] to assume that many organizations have already completed similar projects, but without publishing them in the academic literature.\u201d \nWe\u2019ve already mentioned startups working on this tech, and it\u2019s not hard to find government regimes that would use it. In countries like Iran and Saudi Arabia homosexuality is still punishable by death; in many other countries, being gay means being hounded, imprisoned, and tortured by the state. Recent reports have spoken of the opening of concentration camps for gay men in the Chechen Republic, so what if someone there decides to make their own AI gaydar, and scan profile pictures from Russian social media?\nHere, it becomes clear that the accuracy of systems like Kosinski and Wang\u2019s isn\u2019t really the point. If people believe AI can be used to determine sexual preference, they will use it. With that in mind, it\u2019s more important than ever that we understand the limitations of artificial intelligence, to try and neutralize dangers before they start impacting people. Before we teach machines our prejudices, we need to first teach ourselves. \n"}
