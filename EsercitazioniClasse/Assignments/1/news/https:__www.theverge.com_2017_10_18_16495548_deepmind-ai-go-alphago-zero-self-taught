{"title": "DeepMind\u2019s Go-playing AI doesn\u2019t need human help to beat us anymore", "content": " \nGoogle\u2019s AI subsidiary DeepMind has unveiled the latest version of its Go-playing software, AlphaGo Zero. The new program is a significantly better player than the version that beat the game\u2019s world champion earlier this year, but, more importantly, it\u2019s also entirely self-taught. DeepMind says this means the company is one step closer to creating general purpose algorithms that can intelligently tackle some of the hardest problems in science, from designing new drugs to more accurately modeling the effects of climate change. \nThe original AlphaGo demonstrated superhuman Go-playing ability, but needed the expertise of human players to get there. Namely, it used a dataset of more than 100,000 Go games as a starting point for its own knowledge. AlphaGo Zero, by comparison, has only been programmed with the basic rules of Go. Everything else it learned from scratch. As described in a paper published in Nature today, Zero developed its Go skills by competing against itself. It started with random moves on the board, but every time it won, Zero updated its own system, and played itself again. And again. Millions of times over.\n\u201cWe\u2019ve removed the constraints of human knowledge.\u201d\nAfter three days of self-play, Zero was strong enough to defeat the version of itself that beat 18-time world champion Lee Se-dol, winning handily \u2014 100 games to nil. After 40 days, it had a 90 percent win rate against the most advanced version of the original AlphaGo software. DeepMind says this makes it arguably the strongest Go player in history. \n\u201cBy not using human data \u2014 by not using human expertise in any fashion \u2014 we\u2019ve actually removed the constraints of human knowledge,\u201d said AlphaGo Zero\u2019s lead programmer, David Silver, at a press conference. \u201cIt\u2019s therefore able to create knowledge itself from first principles; from a blank slate [...] This enables it to be much more powerful than previous versions.\u201d\nSilver explained that as Zero played itself, it rediscovered Go strategies developed by humans over millennia. \u201cIt started off playing very naively like a human beginner, [but] over time it played games which were hard to differentiate from human professionals,\u201d he said. The program hit upon a number of well-known patterns and variations during self-play, before developing never-before-seen stratagems. \u201cIt found these human moves, it tried them, then ultimately it found something it prefers,\u201d he said. As with earlier versions of AlphaGo, DeepMind hopes Zero will act as an inspiration to professional human players, suggesting new moves and stratagems for them to incorporate into their game. \n\nAs well as being a better player, Zero has other important advantages compared to earlier versions. First, it needs much less computing power, running on just four TPUs (specialized AI processors built by Google), while earlier versions used 48. This, says Silver, allows for a more flexible system that can be improved with less hassle, \u201cwhich, at the end of the day, is what really matters if we want to make progress.\u201d And second, because Zero is self-taught, it shows that we can develop cutting-edge algorithms without depending on stacks of data.\nFor experts in the field, these developments are a big part of what makes this new research exciting. That\u2019s is because they offer a rebuttal to a persistent criticism of contemporary AI: that much of its recent gains come mostly from cheap computing power and massive datasets. Skeptics in the field like pioneer Geoffrey Hinton suggest that machine learning is a bit of a one-trick pony. Piling on data and compute is helping deliver new functions, but the current pace of advances is unsustainable. DeepMind\u2019s latest research offers something of a rebuttal by demonstrating that there are major improvements to be made simply by focusing on algorithms. \n\u201cThis work shows that a combination of existing techniques can go somewhat further than most people in the field have thought, even though the techniques themselves are not fundamentally new,\u201d Ilya Sutskever, a research director at the Elon Musk-backed OpenAI institute, told The Verge. \u201cBut ultimately, what matters is that researchers keep advancing the field, and it's less important if this goal is achieved by developing radically new techniques, or by applying existing techniques in clever and unexpected ways.\u201d\n\n\n\n\n\n\n\n\nAn earlier version of AlphaGo made headlines when it beat Go champion Lee Se-dol in 2016. That version learned from humans how to play.\nPhoto: Google / Getty Images\n\n\nIn the case of AlphaGo Zero, what is particularly clever is the removal of any need for human expertise in the system. Satinder Singh, a computer science professor who wrote an accompanying article on DeepMind\u2019s research in Nature, praises the company\u2019s work as \u201celegant,\u201d and singles out these aspects. \nSingh tells The Verge that it\u2019s a significant win for the field of reinforcement learning \u2014 a branch of AI in which programs learn by obtaining rewards for reaching certain goals, but are offered no guidance on how to get there. This is a less mature field of work than supervised learning (where programs are fed labeled data and learn from that), but it has potentially greater rewards. After all, the more a machine can teach itself without human guidance, the better, says Singh. \n\u201cOver the past five, six years, reinforcement learning has emerged from academia to have much more broader impact in the wider world, and DeepMind can take some of the credit for that,\u201d says Singh. \u201cThe fact that they were able to build a better Go player here with an order of magnitude less data, computation, and time, using just straight reinforcement learning \u2014 it\u2019s a pretty big achievement. And because reinforcement learning is such a big slice of AI, it\u2019s a big step forward in general.\u201d \nWhat are the applications for these sorts of algorithms? According to DeepMind co-founder Demis Hassabis, they can provide society with something akin to a thinking engine for scientific research. \u201cA lot of the AlphaGo team are now moving onto other projects to try and apply this technology to other domains,\u201d said Hassabis at a press conference. \nDeepMind hopes AlphaGo Zero will be used as an engine for scientific discovery\nHassabis explains that you can think of AlphaGo as essentially a very good machine for searching through complicated data. In the case of Zero, that data is comprised of possible moves in a game of Go. But because Zero was not programmed to understand Go specifically, it could be reprogrammed to discover information in other fields: drug discovery, protein folding, quantum chemistry, particle physics, and material design. \nHassabis suggests that a descendant of AlphaGo Zero could be used to search for a room temperature superconductor \u2014 a hypothetical substance that allows electrical current to flow with zero lost energy, allowing for incredibly efficient power systems. (Superconductors exist, but they only currently work at extremely cold temperatures.) As it did with Go, the algorithm would start by combining different inputs (in this case, the atomic composition of various materials and their associated qualities) until it discovered something humans had missed.\n\n\n\n\n\n\n\n\nDeepMind co-founder Demis Hassabis says his company\u2019s aim is to \u201csolve intelligence.\u201d\nPhoto by Sam Byford / The Verge\n\n\n\u201cMaybe there is a room temperature superconductor out and about. I used to dream about that when I was a kid, looking through my physics books,\u201d says Hassabais. \u201cBut there\u2019s just so many combinations of materials, it\u2019s hard to know whether [such a thing exists].\u201d \nOf course, this would be much more complicated than simply pointing AlphaGo Zero at the Wikipedia page for chemistry and physics and saying \u201chave at it.\u201d Despite its complexity, Go, like all board games, is relatively easy for computers to understand. The rules are finite, there\u2019s no element of luck, no hidden information, and \u2014 most importantly \u2014 researchers have access to a perfect simulation of the game. This means an AI can run millions of tests and be sure it\u2019s not missing anything. Finding other fields that meet these criteria limits the applicability of Zero\u2019s intelligence. DeepMind hasn\u2019t created a magical thinking machine. \nThese caveats aside, the research published today does get DeepMind just a little bit closer to solving the first half of its tongue-in-cheek, two-part mission statement. Part one: solve intelligence; part two: use it to make the world a better place. \u201cWe\u2019re trying to build general purpose algorithms and this is just one step towards that, but it\u2019s an exciting step,\u201d says Hassabis. \n\n"}
